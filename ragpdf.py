# -*- coding: utf-8 -*-
"""RagPDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwwjWnW8aZeg7WnA1510MUBBAZ5u7R_o
"""

! pip install langchain_community tiktoken langchain_openai langchainhub chromadb langchain
! pip install -q -U google-generativeai
! pip install langchain_google_genai
! pip install gpt4all
! pip install pypdf2



import os
os.environ['LANGCHAIN_TRACING_V2'] = 'true'
os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
os.environ['LANGCHAIN_API_KEY'] = 'ls__5e058a833a4d42369a65587f6432fc25'
os.environ['GOOGLE_API_KEY'] ='AIzaSyDeQAgxtiV3Sre1-oqqySzDATq5St6n2zk'

from PyPDF2 import PdfReader
import bs4
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document
#### INDEXING ####
def load_pdf(pdf_path):
    try:
        with open(pdf_path, "rb") as file:
            reader = PdfReader(file)
            text = ""
            for page_num in range(len(reader.pages)):
                page = reader.pages[page_num]
                text += page.extract_text()
            return text
    except FileNotFoundError:
        print(f"Error: PDF file not found at {pdf_path}")
        return None

# Define the path of the PDF file
pdf_path = "/content/drive/MyDrive/collab/A_Brief_Introduction_To_AI.pdf"

# Load PDF text from the path
pdf_text = load_pdf(pdf_path)

# Create a Document object from the PDF text
docs = [Document(page_content=pdf_text, metadata={'source': pdf_path})]
# Split
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# Embed
vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())

retriever = vectorstore.as_retriever()

#### RETRIEVAL and GENERATION ####

# Prompt
prompt = hub.pull("rlm/rag-prompt")

# LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)

# Post-processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
# Question
rag_chain.invoke("What is Artificial Intelligence?")

# Documents
question = "What is Artificial Intelligence?"
document = "A_Brief_Introduction_To_AI"

import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

num_tokens_from_string(question, "cl100k_base")

from langchain_community.embeddings import GPT4AllEmbeddings
embd = GPT4AllEmbeddings()
query_result = embd.embed_query(question)
document_result = embd.embed_query(document)
len(query_result)

import numpy as np

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

similarity = cosine_similarity(query_result, document_result)
print("Cosine Similarity:", similarity)

#### INDEXING ####

# Load blog
import bs4
from langchain_community.document_loaders import WebBaseLoader
loader = load_pdf(pdf_path)
blog_docs = [Document(page_content=pdf_text, metadata={'source': pdf_path})]

# Split
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=300,
    chunk_overlap=50)

# Make splits
splits = text_splitter.split_documents(blog_docs)

# Index
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())

retriever = vectorstore.as_retriever()

# Index
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())

retriever = vectorstore.as_retriever(search_kwargs={"k": 1})

docs = retriever.get_relevant_documents("What is Artificial Intelligence?")

len(docs)

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

# Prompt
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
prompt

# LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)

# Chain
chain = prompt | llm

# Run
chain.invoke({"context":docs,"question":"What is Artificial Intelligence?"})

from langchain import hub
prompt_hub_rag = hub.pull("rlm/rag-prompt")

prompt_hub_rag

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What is Artificial Intelligence?")

class FeedbackLoop:
    def __init__(self, model):
        self.model = model

    def generate_response(self, context, question):
            template = """Answer the question based only on the following context:
            {context}

            Question: {question}
            """

            prompt = ChatPromptTemplate.from_template(template)
            prompt
            chain = prompt | llm
            response = chain
            feedback = input("Was the response helpful? (yes/no): ")
            if feedback.lower() == "yes":
                # If the user found the response helpful, no action needed
                print("Thank you for your feedback!")
            elif feedback.lower() == "no":
              # If the user didn't find the response helpful, collect additional information
              new_question = input("What would you like to improve?")
              print("Thank you for your feedback! We'll try to improve the response.")
            else:
                print("Invalid feedback. Please respond with 'yes' or 'no'.")

            return response

feedback_loop = FeedbackLoop(llm)
context=retriever
response = feedback_loop.generate_response(context, question)